{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GitHub Policy Dashboard","text":""},{"location":"#overview","title":"Overview","text":"<p>The GitHub Policy Dashboard is a tool designed to monitor ONS' adherence to the GitHub Usage Policy. The Dashboard provides insights into 3 main areas:</p> <ul> <li>Repository Compliance</li> <li>Secret Scanning Alert Metrics</li> <li>Dependabot Alert Metrics</li> </ul> <p>The dashboard is used as a part of the ONSdigital audit process to ensure compliance and security across the GitHub Organisation.</p>"},{"location":"#scope","title":"Scope","text":"<p>The dashboard provides a utility to capture the state of ONSdigital at a given point in time. The dashboard is not intended to be a live monitoring tool, but rather a snapshot of the current state of the ONSdigital GitHub Organisation.</p> <p>The information provided by the dashboard is intended to be used as a guide for ONSdigital teams to ensure compliance with the GitHub Usage Policy. The dashboard is not intended to be a comprehensive security tool, but rather a tool to help teams identify areas for improvement, GitHub Owners and Administrators to encourage improvement and to provide stakeholders with assurance that ONSdigital is adhering to the GitHub Usage Policy.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The GitHub Policy Dashboard is built using the following components:</p> <ul> <li>Policy Dashboard: A Python Streamlit application that provides the user interface for the dashboard.</li> <li>Data Logger: An AWS Lambda function that collects data from the GitHub API and stores it in an S3 bucket. This collects the data required for the dashboard to function.</li> </ul> <p>The below diagram illustrates the architecture of the GitHub Policy Dashboard and how the components interact with each other.</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Getting started guides are available in the project's READMEs:</p> <ul> <li>Policy Dashboard</li> <li>Data Logger</li> </ul>"},{"location":"#additional-information","title":"Additional Information","text":"<p>For more information about each service, please refer to their individual documentation pages:</p> <ul> <li>Policy Dashboard</li> <li>Data Logger</li> </ul>"},{"location":"documentation/","title":"Documentation","text":"<p>This site uses MkDocs to build its documentation and GitHub Pages for hosting.</p>"},{"location":"documentation/#format","title":"Format","text":"<p>Documentation within this project follows the following pattern:</p> <ul> <li>A <code>README.md</code> for each component</li> <li>A <code>/docs</code> folder for the project</li> </ul> <p>Each <code>README.md</code> should contain:</p> <ul> <li>A description of what the component is/does</li> <li>A list of any prerequisites</li> <li>Setup instructions</li> <li>Execution instructions</li> <li>Deployment instructions</li> </ul> <p>The <code>/docs</code> folder should contain:</p> <ul> <li>A description of what the project is</li> <li>An overview of how the everything fits together in the project</li> <li>An explanation of the tech stack</li> <li>Details of the underlying dataset</li> </ul> <p>A majority of the information should reside within the <code>/docs</code> directory over the <code>README</code>. The <code>README</code>s in this project should be kept for concise instructions on how to use each component. Any detailed explanation should be kept within <code>/docs</code>.</p>"},{"location":"documentation/#getting-mkdocs-setup","title":"Getting MkDocs Setup","text":"<p>In order to build an MkDocs deployment or serve the documentation locally, we need to install MkDocs and its dependencies.</p> <ol> <li> <p>Navigate into the project's root directory.</p> </li> <li> <p>Install MkDocs and its dependencies.</p> <pre><code>pip install -r mkdocs_requirements.txt\n</code></pre> </li> <li> <p>You can now use MkDocs. To see a list of commands run the following:</p> <pre><code>mkdocs --help\n</code></pre> </li> </ol> <p>Please Note: Python's package manager, PIP, is required to install MkDocs. Please make sure you have Python installed beforehand.</p>"},{"location":"documentation/#updating-mkdocs-deployment","title":"Updating MkDocs Deployment","text":""},{"location":"documentation/#github-action-to-deploy-documentation","title":"GitHub Action to Deploy Documentation","text":"<p>A GitHub Action is set up to automatically deploy the documentation to GitHub Pages whenever a commit is made to the <code>main</code> branch. This action is triggered by a push event to the <code>main</code> branch and runs the <code>mkdocs gh-deploy</code> command to build and deploy the documentation.</p>"},{"location":"documentation/#manual-deployment","title":"Manual Deployment","text":"<p>If changes are made within <code>/docs</code>, the GitHub Pages deployment will need to be updated. Assuming you have already installed MkDocs and Material for MkDocs, do the following:</p> <ol> <li> <p>Navigate to the projects root directory.</p> </li> <li> <p>Deploy the documentation to GitHub Pages.</p> <pre><code>mkdocs gh-deploy\n</code></pre> </li> <li> <p>This will build the documentation and deploy it to the <code>gh-pages</code> branch of your repository. The documentation will be available at <code>https://ONS-Innovation.github.io/&lt;repository-name&gt;/</code>.</p> </li> </ol> <p>Please Note: The <code>gh-deploy</code> command will overwrite the <code>gh-pages</code> branch and make the local changes available on GitHub Pages. Make sure that these changes are appropriate and have been reviewed before deployment.</p>"},{"location":"dashboard/","title":"Dashboard","text":""},{"location":"dashboard/#overview","title":"Overview","text":"<p>The Dashboard is a Python Streamlit application that provides the user interface for the GitHub Policy Dashboard. It is designed to display the data collected by the Data Logger and provide insights into ONSdigital's adherence to the GitHub Usage Policy.</p>"},{"location":"dashboard/#features","title":"Features","text":""},{"location":"dashboard/#repository-analysis","title":"Repository Analysis","text":"<p>Displays the compliance status of repositories against the GitHub Usage Policy.</p> <ul> <li>Filter Repositories: Allows users to filter repositories based on a range of parameters.</li> <li>Rule Presets: Provides predefined sets of rules for quick compliance checks.</li> <li>Display Rule Details: Shows detailed information about each rule - promoting better understanding of compliance.</li> <li>Organisation Overview: Offers a high-level view of the organisation's compliance status, including which rule is broken the most.</li> <li>Repository Details: Provides detailed information about individual repositories, including their compliance status and the rules they violate.</li> <li>Point of Contact: Displays the point of contact for each repository, facilitating communication regarding compliance issues.</li> </ul>"},{"location":"dashboard/#secret-scanning","title":"Secret Scanning","text":"<p>Provides metrics on Secret Scanning alerts.</p> <ul> <li>Filter Alerts: Allows users to filter alerts based on a range of parameters.</li> <li>Organisation Overview: Offers a high-level view of the organisation's secret scanning alerts, including the oldest open alert.</li> <li>Respository Proportion: Displays the proportion of repositories with secret scanning alerts - allowing users to highlight the organisation's worst repositories.</li> <li>Alert Details: Shows information about individual alerts, promoting users to take action on them.</li> </ul>"},{"location":"dashboard/#dependabot-alerts","title":"Dependabot Alerts","text":"<p>Displays metrics on Dependabot alerts.</p> <ul> <li>Filter Alerts: Allows users to filter alerts based on a range of parameters.</li> <li>Organisation Overview: Offers a high-level view of the organisation's Dependabot alerts, including the oldest open alert and worst severity.</li> <li>Severity Proportion: Displays the proportion of alerts by severity, allowing users to focus on the most critical issues.</li> <li>Repository Overview: Provides a high-level view of the repositories with Dependabot alerts, including the number of alerts and their severity.</li> <li>Repository Severity Proportion: Displays the proportion of alerts by severity for each repository, allowing users to identify the most critical issues.</li> </ul>"},{"location":"dashboard/#data-refreshing","title":"Data Refreshing","text":"<ul> <li>Users can refresh the backend data manually by clicking the \"Refresh Data\" button in the sidebar. This will trigger the Data Logger to collect the latest data from GitHub and update the S3 bucket. This functionality is considerate of GitHub's API rate limits, ensuring that there is enough rate limit remaining before attempting to refresh the data. If the rate limit is exceeded, the user will be informed and the refresh will not proceed.</li> </ul>"},{"location":"dashboard/#data-collection-process","title":"Data Collection Process","text":"<p>The Dashboard relies on the data collected by the Data Logger, which is responsible for gathering information from GitHub repositories and storing it in a database. The Data Logger runs periodically to ensure that the Dashboard has up-to-date information.</p> <p>The Dashboard collects its data from an S3 Bucket on AWS, which is populated by the Data Logger. The bucket contains JSON files for repository, Secret Scanning, and Dependabot data. The Dashboard reads these files to display the relevant information.</p>"},{"location":"dashboard/#diagram-of-data-flow","title":"Diagram of Data Flow","text":"<pre><code>---\nTitle: Data Process from S3 to the user\n---\ngraph TD\n  A[S3 Bucket] --&gt;|JSON Files| B[Dashboard];\n  B --&gt;|JSON Data| C[Convert to&lt;br&gt; Pandas DataFrames];\n  C --&gt;|Pandas DataFrames| D[Add Additional Columns];\n  D --&gt;|Enhanced DataFrames| E[Filter DataFrames];\n  E --&gt;|Filtered DataFrames| F[Add Additional&lt;br&gt; Calculations];\n  F --&gt;|Final Data| G{Does the data&lt;br&gt; need to be grouped?};\n  G --&gt;|Yes| H[Group Data];\n  G --&gt;|No| I[Display Data];\n  H --&gt;|Grouped Data| I;\n  I --&gt;|Visualisations| J[User];</code></pre>"},{"location":"dashboard/#formatting-and-filtering","title":"Formatting and Filtering","text":"<p>To make the data more user-friendly, the Dashboard applies formatting and filtering to the JSON data. The processing includes:</p> <ul> <li>Pandas DataFrames: The JSON data is converted into Pandas DataFrames for easier manipulation and analysis.</li> <li>Additional Columns: New columns are added to the DataFrames to provide more context and insights, such as each repositories' visibility (See Repository Data Collection).</li> <li>Filtering: The DataFrames are filtered based on user input, allowing users to focus on specific repositories or alerts.</li> <li>Grouping: In areas, the initial DataFrames get grouped by repository or severity. The tool stores the grouped data in a new DataFrame so that the original DataFrame remains unchanged for further analysis.</li> </ul>"},{"location":"dashboard/#caching","title":"Caching","text":"<p>In order to improve performance and reduce wait times in the frontend, the dashboard makes use of Streamlit's caching. Documentation on this is available within Streamlit's Documentation.</p> <p>Each function used to collect, filter or format data likely uses the <code>@st.cache_data</code> decorator. This works by checking function parameters and if the function has already been called with those parameters, a cached output of the function will be used instead of rerunning it.</p> <p>Each cached function has been given a time to live (ttl) value of an hour. This value ensures that cache is used as much as possible to improve performance, while still running the functions often enough that the data doesn't become outdated. Users are unlikely to use the dashboard for more than an hour at a time.</p>"},{"location":"dashboard/#rule-logic","title":"Rule Logic","text":"<p>In order for the dashboard to provide additional information about the rules, <code>rulemap.json</code> is used. More information about this file can be found in Rule Mapping, including how to add new rules and changing the presets.</p>"},{"location":"dashboard/repository_information/","title":"Repository Data Collection","text":""},{"location":"dashboard/repository_information/#overview","title":"Overview","text":"<p>Dependabot and Secret Scanning alerts are collected from their respective GitHub API Endpoints.</p> <ul> <li>Secret Scanning: <code>GET /orgs/{org}/secret-scanning/alerts</code></li> <li>Dependabot: <code>GET /orgs/{org}/dependabot/alerts</code></li> </ul> <p>These endpoints list all alerts for the organisation. The endpoint contains the name of the repository they belong to, but, unfortunately, does not contain the repository's visibility (public/private/internal) or if the repository is archived or not.</p> <p>This information is important to allow users to filter alerts appropriately, for example, any public Secret Scanning alerts are much more of a risk than private ones. We want users to be able to highlight these sorts of issues.</p> <p>We must, therefore, collect the repository information from the GitHub API separately to allow us to provide this functionality.</p>"},{"location":"dashboard/repository_information/#how-is-the-data-collected","title":"How is the data collected?","text":"<p>The additional repository information is collected by the dashboard at runtime. Although this is not ideal due to performance, it must be collected in the frontend due to the already stretched rate limits of the GitHub API in the Data Logger.</p> <p>In order to make this process as efficient and performant as possible, the function changes how it collects the data based on which alerts the data is being collected for.</p> <p>The function is available within <code>./src/utilities</code> as <code>get_github_repository_information()</code>. See below for the function's docstring.</p> <p>Retrieves additional information about repositories in a GitHub organization (Repository Type and Archived Status).</p> <p>Parameters:</p> Name Type Description Default <code>ql</code> <code>github_graphql_interface</code> <p>The GraphQL interface for GitHub API.</p> required <code>org</code> <code>str</code> <p>The GitHub organization name.</p> required <code>repository_list</code> <code>list</code> <p>A list of specific repositories to check. If None, all repositories in the organization are checked.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>Tuple[dict, dict]: A tuple containing two dictionaries: - repo_types: A dictionary mapping repository names to their types (Public, Internal, Private). - archived_status: A dictionary mapping repository names to their archived status (Archived, Not Archived).</p> Source code in <code>src/utilities.py</code> <pre><code>@st.cache_data(ttl=timedelta(hours=1))\ndef get_github_repository_information(\n    _rest: github_api_toolkit.github_interface, \n    org: str, \n    repository_list: list = None\n) -&gt; Tuple[dict, dict]:\n    \"\"\"Retrieves additional information about repositories in a GitHub organization (Repository Type and Archived Status).\n\n    Args:\n        ql (github_api_toolkit.github_graphql_interface): The GraphQL interface for GitHub API.\n        org (str): The GitHub organization name.\n        repository_list (list, optional): A list of specific repositories to check. If None, all repositories in the organization are checked.\n\n    Returns:\n        Tuple[dict, dict]: A tuple containing two dictionaries:\n            - repo_types: A dictionary mapping repository names to their types (Public, Internal, Private).\n            - archived_status: A dictionary mapping repository names to their archived status (Archived, Not Archived).\n    \"\"\"\n\n    if repository_list:\n        # If a specific list of repositories is provided, retrieve their types\n        # This is useful since Secret Scanning will only return a handful of repositories\n\n        repo_types = {}\n        archived_status = {}\n\n        for repo in repository_list:\n            response = _rest.get(f\"/repos/{org}/{repo}\")\n\n            if type(response) is not Response:\n                print(f\"Error retrieving repository {repo}: {response}\")\n                repo_types[repo] = \"Unknown\"\n                archived_status[repo] = \"Unknown\"\n            else:\n                repository = response.json()\n                repository_type = repository.get(\"visibility\", \"Unknown\").title()\n                repo_types[repo] = repository_type\n\n                archived_status[repo] = \"Archived\" if repository.get(\"archived\", False) else \"Not Archived\"\n\n    else:\n        # If no specific list is provided, retrieve all repositories in the organization\n        # This is useful for Dependabot Alerts where there are many repositories\n        # There will be less API calls doing 100 repositories at a time than each repository individually\n\n        repo_types = {}\n        archived_status = {}\n        repository_list = []\n\n        response = _rest.get(f\"/orgs/{org}/repos\", params={\"per_page\": 100})\n\n        if type(response) is not Response:\n            print(f\"Error retrieving repositories: {response}\")\n            return repo_types, archived_status\n        else:\n            try:\n                last_page = int(response.links[\"last\"][\"url\"].split(\"=\")[-1])\n            except KeyError:\n                last_page = 1\n\n        for page in range(1, last_page + 1):\n            response = _rest.get(f\"/orgs/{org}/repos\", params={\"per_page\": 100, \"page\": page})\n\n            if type(response) is not Response:\n                print(f\"Error retrieving repositories on page {page}: {response}\")\n                continue\n\n            repositories = response.json()\n\n            repository_list = repository_list + repositories\n\n        for repo in repository_list:\n            repository_name = repo.get(\"name\")\n            repository_type = repo.get(\"visibility\", \"Unknown\").title()\n            repo_types[repository_name] = repository_type\n\n            archived_status[repository_name] = \"Archived\" if repo.get(\"archived\", False) else \"Not Archived\"\n\n    return repo_types, archived_status\n</code></pre> <p>The function works by collecting the repository information of either all repositories in the organisation or only those that have been passed to the function within <code>repository_list</code>. The option is provided because in some cases, it requires less API calls to collect all repositories in the organisation, rather than just those that have alerts.</p> <p>For Secret Scanning alerts, there are likely to be less than 30 repositories with alerts, so it is more efficient to collect only those repositories. For Dependabot alerts, there are likely to be more than 30 repositories with alerts, so it is more efficient to collect all repositories in the organisation.</p> <p>It works out cheaper on the API to collect all repositories in the organisation because we can collect up to 100 repositories per API call. At the time of writing, the organisation has around 3000 repositories, meaning 30 API calls to collect all repositories. If there are more than 30 repositories to collect, we may as well collect all repositories in the organisation, as it will only require 30 API calls.</p> <p>The endpoints used to collect the repository information are:</p> <ul> <li>List Repositories for an Organisation: <code>GET /orgs/{org}/repos</code></li> <li>Get a Repository: <code>GET /repos/{owner}/{repo}</code></li> </ul>"},{"location":"dashboard/repository_information/#how-is-the-data-used","title":"How is the data used?","text":"<p>Once we have the repository information, we can map it to a new column within the DataFrame containing the respective alerts - providing the extra data we need.</p>"},{"location":"dashboard/rulemap/","title":"Rule Mapping","text":""},{"location":"dashboard/rulemap/#overview","title":"Overview","text":"<p>In order to add more information about the rules applied to repositories, <code>rulemap.json</code> is used. This allows the tool to connect each check in <code>repositories.json</code> with additional information about the rule, such as a description and the preset it applies to.</p>"},{"location":"dashboard/rulemap/#where-do-the-rules-come-from","title":"Where do the rules come from?","text":"<p>All rules on the dashboard are derived from ONS' GitHub Usage Policy.</p>"},{"location":"dashboard/rulemap/#format","title":"Format","text":"<p>The <code>rulemap.json</code> file is a JSON object containing an array of rule objects. Each rule object has the following structure:</p> <pre><code>[\n    {\n        \"name\": \"String\",\n        \"description\": \"String\",\n        \"is_security_rule\": bool,\n        \"is_policy_rule\": bool,\n        \"note\": \"String\"\n    }\n]\n</code></pre>"},{"location":"dashboard/rulemap/#fields","title":"Fields","text":"Field Type Description <code>name</code> String The name of the rule. This should match the name used in <code>repositories.json</code>. <code>description</code> String A detailed description of the rule, explaining what it checks and when it applies. <code>is_security_rule</code> bool Indicates if the rule is a security rule. If true, the rule will be included in the Security Preset. <code>is_policy_rule</code> bool Indicates if the rule is a policy rule. If true, the rule will be included in the Policy Preset. <code>note</code> String Additional notes or comments about the rule. This is mainly used to explain what happens to repositories that the check doesn't run on (i.e. if a check is only for public repositories, this rule would explain that private/internal repositories automatically pass the check)."},{"location":"dashboard/rulemap/#example","title":"Example","text":"<pre><code>[\n    {\n        \"name\": \"codeowners_missing\",\n        \"description\": \"The repository does not have a CODEOWNERS file.\",\n        \"is_security_rule\": false,\n        \"is_policy_rule\": true,\n        \"note\": \"\"\n    },\n    {\n        \"name\": \"point_of_contact_missing\",\n        \"description\": \"A contact email address cannot be found from the CODEOWNERS file.\",\n        \"is_security_rule\": true,\n        \"is_policy_rule\": true,\n        \"note\": \"This rule will only check if a point of contact email address can be found from the CODEOWNERS file. If the CODEOWNERS file is missing, this rule will not be triggered and be marked as compliant.\"\n    }\n]\n</code></pre> <p>More example can be found in the file itself, which is located in the root of the repository.</p>"},{"location":"dashboard/rulemap/#adding-rules","title":"Adding Rules","text":""},{"location":"dashboard/rulemap/#backend-changes","title":"Backend Changes","text":"<p>First you must create the new rule in the Data Logger. In <code>data_logger/src/main.py</code>, there is a function called <code>get_repository_batch()</code>. This function is where the final data structure of <code>repositories.json</code> is created. You can add your new rule to the <code>repository_data</code> dictionary in this function and put the logic in its own function (if possible in <code>policy_checks.py</code>).</p> <p>Once this has been done, the new rule will be collected by the Data Logger and added to the <code>repositories.json</code> file when it runs.</p>"},{"location":"dashboard/rulemap/#frontend-changes","title":"Frontend Changes","text":"<p>Once the backend changes have been made, you need to flesh out the rule in the <code>rulemap.json</code> file. You can do this by adding a new object to the array with the appropriate fields filled out. Make sure that the <code>name</code> field matches the name of the key used in <code>repositories.json</code>.</p> <p>i.e.</p> <p>In <code>repositories.json</code> you might have:</p> <pre><code>repository_data = {\n    \"name\": repository[\"name\"],\n    \"type\": repository[\"visibility\"],\n    \"url\": repository[\"url\"],\n    \"created_at\": repository[\"createdAt\"],\n    \"checklist\": {\n        \"example_check\": do_check()\n    }\n}\n</code></pre> <p>Then in <code>rulemap.json</code>, you would add:</p> <pre><code>{\n    \"name\": \"example_check\", &lt;!-- This MUST match the key in repositories.json --&gt;\n    \"description\": \"This is an example check that does something.\",\n    \"is_security_rule\": false,\n    \"is_policy_rule\": true,\n    \"note\": \"\"\n}\n</code></pre> <p>A new check should now appear in the dashboard under the appropriate preset.</p>"},{"location":"dashboard/rulemap/#changing-rule-presets","title":"Changing Rule Presets","text":"<p>To change which preset a rule belongs to, you can modify the <code>is_security_rule</code> and <code>is_policy_rule</code> fields in the <code>rulemap.json</code> file. If a rule is marked as a security rule, it will be included in the Security Preset. If it is marked as a policy rule, it will be included in the Policy Preset.</p>"},{"location":"data_logger/","title":"Data Logger","text":""},{"location":"data_logger/#overview","title":"Overview","text":"<p>The Data Logger is a Python script that collects data from the GitHub API and stores it in an S3 Bucket as JSON. It is designed to be run periodically, currently weekly, to ensure that the data is up-to-date. Since the scope of the dashboard is only to be used for snapshots and regular audits, the update frequency is not critical.</p>"},{"location":"data_logger/#data-collection-process","title":"Data Collection Process","text":"<p>The Data Logger interacts with the GitHub API to gather data for a specific organinsation. Once collected from the API, the data is stored within an S3 bucket in JSON format. Whether the data is stored into S3 or not is controlled within the <code>config.json</code> file, allowing the tool to be debugged locally. For more information on the configuration, see the Configuration page.</p> <p>Once the data has been written to S3, the Dashboard can access it freely.</p>"},{"location":"data_logger/#diagram-of-data-flow","title":"Diagram of Data Flow","text":"<p>The below diagram shows the collection process for a single type of data, such as <code>repositories.json</code>. For a more detailed view of each data type, see the respective pages:</p> <ul> <li><code>repositories.json</code>: Data Collection &gt; Repositories</li> <li><code>secret_scanning.json</code>: Data Collection &gt; Secret Scanning</li> <li><code>dependabot.json</code>: Data Collection &gt; Dependabot</li> </ul> <pre><code>graph TD\n    A{Is the collection&lt;br&gt; for this type&lt;br&gt; enabled?}\n    A --&gt;|Yes| B[Collect Data from&lt;br&gt; the GitHub API]\n    B --&gt;|API Response| C[Get Response Data&lt;br&gt; as a Dictionary]\n    C --&gt;|Dictionary| D[Write to S3 Bucket&lt;br&gt; as JSON]\n    D --&gt;|Data Available for Dashboard| E[Dashboard];\n    A --&gt;|No| F[Skip Collection]</code></pre>"},{"location":"data_logger/#collection-frequency","title":"Collection Frequency","text":"<p>The Data Logger is currently set to run weekly. This frequency is sufficient for the dashboard's purpose of providing snapshots and regular audits. The frequency can be adjusted using Terraform.</p> <p>The Lambda is triggered by a CloudWatch Event Rule (EventBridge Trigger) which follows a cron expression. This cron expression gets passed within the <code>.tfvars</code> file during the deployment of the Data Logger. Simply change the cron expression to adjust the frequency of the data collection.</p> <p>When adjusting the frequency, consider the following:</p> <ul> <li>API Rate Limits: The Data Logger must run at a time where no other tools are using the GitHub API to avoid hitting rate limits (providing they share the same GitHub App).</li> <li>Cost: Only run the Data Logger as frequently as necessary to keep costs down. Once a week is typically sufficient for most use cases.</li> <li>Data Freshness: Ensure that the data remains relevant and up-to-date for the dashboard's needs. Weekly updates are generally adequate for snapshot and audit purposes.</li> <li>Outside of Working Hours: If the Data Logger is run during working hours, the data being collected may change while the Data Logger is running. This can lead to inconsistencies in the data collected - especially with <code>repositories.json</code> - where multiple repositories within the organisation get updated per second.</li> </ul>"},{"location":"data_logger/#hardware-deployment-requirements","title":"Hardware / Deployment Requirements","text":"<p>The Data Logger is designed to run as an AWS Lambda function. This means that the process must be completed in less than 15 minutes, which is the maximum execution time for a Lambda function. A few performance considerations have been considered to ensure that the Data Logger runs efficiently within this time limit:</p> <ul> <li>Threading: The Data Logger uses Python's threading capabilities to run multiple API calls in parallel. This allows for faster data collection, especially when dealing with large organisations with many repositories. More on the use of threading can be found in the Threading page.</li> <li>Allocated Memory: Depending on the size of the organisation, the Lambda function may require more memory to run efficiently. AWS Lambda scales the CPU power allocated to the function based on the amount of memory allocated. Therefore, increasing the memory allocation can lead to faster execution times.</li> <li>If working with ONSdigital, more memory is required than if working with ONS-Innovation. This is due to the larger volume of data being collected from the ONSdigital organisation. The current recommended memory allocation for each environment is noted in the README file for the Data Logger.</li> <li>Timeout Settings: The Lambda function is set to a timeout of 15 minutes, which is the maximum allowed. This ensures that the Data Logger has enough time to complete its tasks without being prematurely terminated.</li> </ul>"},{"location":"data_logger/configuration/","title":"Configuration","text":""},{"location":"data_logger/configuration/#overview","title":"Overview","text":"<p>The Data Logger makes use of a <code>config.json</code> file to control various aspects of its operation. This file gets deployed within the container image, requiring redeployment should any run parameters need to be changed.</p>"},{"location":"data_logger/configuration/#configuration-for-local-development","title":"Configuration for Local Development","text":"<p>To test the Data Logger locally, the following <code>features</code> should be set accordingly:</p> <pre><code>{\n    \"features\": {\n        \"repository_collection\": true,\n        \"dependabot_collection\": true,\n        \"secret_scanning_collection\": true,\n        \"show_log_locally\": true,\n        \"write_to_s3\": false // This MUST be false or else the live data will be overwritten\n    },\n    \"settings\": {\n        ... // Other settings as required\n    }\n}\n</code></pre> <p>The different collection features can be toggled on or off to control which data is collected. If only making changes to a single collection type, you can set the others to <code>false</code> to speed up the local testing process.</p>"},{"location":"data_logger/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"data_logger/configuration/#features","title":"Features","text":"<p>The features section allows developers to toggle various aspects of the Data Logger's operation to their needs.</p> <pre><code>{\n    \"features\": {\n        \"repository_collection\": true,\n        \"dependabot_collection\": true,\n        \"secret_scanning_collection\": true,\n        \"show_log_locally\": true,\n        \"write_to_s3\": true\n    },\n    \"settings\": {\n        ... // Other settings as required\n    }\n}\n</code></pre>"},{"location":"data_logger/configuration/#repository-dependabot-and-secret-scanning-collection","title":"Repository, Dependabot, and Secret Scanning Collection","text":"<p>These features control whether the Data Logger collects data for the respective types. If set to <code>true</code>, the Data Logger will collect data for that key, otherwise it will skip the collection process for that type.</p> <p>When deploying to AWS, all of these features should be set to <code>true</code> to ensure that the Data Logger collects all necessary data for the dashboard.</p> <p>Setting these features to <code>false</code> can help speed up local testing and debugging, as it reduces the amount of data being collected and processed.</p>"},{"location":"data_logger/configuration/#show-log-locally","title":"Show Log Locally","text":"<p>This feature controls whether the Data Logger outputs logs to a local text file. When set to <code>true</code>, the Data Logger will write logs to a file in the local directory, which can be useful for debugging and testing purposes, otherwise it will not write logs locally. This can help developers see the output of the Data Logger as if looking at the CloudWatch logs in AWS.</p>"},{"location":"data_logger/configuration/#write-to-s3","title":"Write to S3","text":"<p>This feature controls whether the Data Logger writes the collected data to AWS S3. When set to <code>true</code>, the Data Logger will write the collected data to the specified S3 bucket in JSON format. If set to <code>false</code>, the Data Logger will instead write the data to a local file for developers to inspect. This is particularly useful for debugging and testing purposes, as it allows developers to see the data that would be written to S3 without actually modifying the live data.</p> <p>When developing locally, this must be set to <code>false</code> to prevent overwriting the live data in S3. If for some reason you need to write to S3 while developing locally (i.e. to test the data with the dashboard), you should ensure that other team members are aware and that the data is not critical, as it will overwrite the existing data in S3.</p>"},{"location":"data_logger/configuration/#settings","title":"Settings","text":"<p>The settings section contains various parameters that control the behaviour of the Data Logger, including when checks are considered to be breaches of policy. It is highly unlikely that these settings will need to be changed - unless ONS' GitHub Usage Policy changes - but they are included here for completeness.</p> <pre><code>{\n    \"features\": {\n        ... // Feature settings as above\n    },\n    \"settings\": {\n        \"thread_count\": 20,\n        \"dependabot_thresholds\": {\n            \"critical\": 5,\n            \"high\": 15,\n            \"medium\": 60,\n            \"low\": 90\n        },\n        \"secret_scanning_threshold\": 5,\n        \"inactivity_threshold\": 1,\n        \"signed_commit_number\": 15\n    }\n}\n</code></pre>"},{"location":"data_logger/configuration/#thread-count","title":"Thread Count","text":"<p>This setting controls the number of threads used by the Data Logger when collecting data from the GitHub API. Increasing this number can speed up data collection, especially for large organisations with many repositories. However, it also increases the load on the GitHub API, so it should be set to a reasonable value to avoid hitting rate limits.</p> <p>For more information on how threading is used in the Data Logger, see the Threading page.</p>"},{"location":"data_logger/configuration/#dependabot-thresholds","title":"Dependabot Thresholds","text":"<p>These thresholds control how many days an alert must be open before it is considered to be a breach of policy. The thresholds are set for each severity level of Dependabot alerts and are derived from ONS' GitHub Usage Policy.</p>"},{"location":"data_logger/configuration/#secret-scanning-threshold","title":"Secret Scanning Threshold","text":"<p>This threshold controls how many days a secret scanning alert must be open before it is considered to be a breach of policy. This is derived from ONS' GitHub Usage Policy.</p>"},{"location":"data_logger/configuration/#inactivity-threshold","title":"Inactivity Threshold","text":"<p>This threshold controls how many years a repository can go without updates before it is considered inactive. This is derived from ONS' GitHub Usage Policy.</p>"},{"location":"data_logger/configuration/#signed-commit-number","title":"Signed Commit Number","text":"<p>This setting controls how many commits are checked when applying the signed commit policy check. This value has been agreed between stakeholders.</p>"},{"location":"data_logger/dependabot/","title":"Dependabot","text":""},{"location":"data_logger/dependabot/#overview","title":"Overview","text":"<p>This dataset contains information about the Dependabot alerts within the organisation. The dataset does not contain any sensitive information such as the content of the alerts, only that there is an alert and the repository it is associated with. Dependabot alerts must be past a threshold to be included in this dataset. The thresholds for each alert severity are defined in the Data Logger's configuration file (See Configuration for more).</p> <p>This dataset includes all open alerts within the thresholds, regardless of whether the repository is archived or not. You can filter alerts by this in the frontend. Information about whether an alert's associated repository is archived and its visibility is collected in the frontend rather than here (See Repository Data Collection for more).</p>"},{"location":"data_logger/dependabot/#structure","title":"Structure","text":"<pre><code>[\n    {\n        \"repository\": \"{repo}\",\n        \"repository_url\": \"https://github.com/{org}/{repo}\",\n        \"created_at\": \"2024-12-11T23:54:00Z\",\n        \"severity\": \"critical | high | medium | low\",\n    },\n]\n</code></pre>"},{"location":"data_logger/repositories/","title":"Repositories","text":""},{"location":"data_logger/repositories/#overview","title":"Overview","text":"<p>This dataset contains information about repositories within the organisation. It's primary purpose is to monitor repository compliance with the GitHub Usage Policy. This dataset does not contain any archived repositories as these are not active and considered out of scope.</p>"},{"location":"data_logger/repositories/#structure","title":"Structure","text":"<pre><code>[\n    {\n        \"name\": \"{repo}\",\n        \"type\": \"PUBLIC | PRIVATE | INTERNAL\",\n        \"url\": \"https://github.com/{org}/{repo}\",\n        \"created_at\": \"2023-12-04T14:33:57Z\",\n        \"checklist\": {\n            \"inactive\": true | false,\n            \"unprotected_branches\": true | false,\n            \"unsigned_commits\": true | false,\n            \"readme_missing\": true | false,\n            \"license_missing\": true | false,\n            \"pirr_missing\": true | false,\n            \"gitignore_missing\": true | false,\n            \"external_pr\": true | false,\n            \"breaks_naming_convention\": true | false,\n            \"secret_scanning_disabled\": true | false,\n            \"push_protection_disabled\": true | false,\n            \"dependabot_disabled\": true | false,\n            \"codeowners_missing\": true | false,\n            \"point_of_contact_missing\": true | false\n        }\n    },\n]\n</code></pre>"},{"location":"data_logger/secret_scanning/","title":"Secret Scanning","text":""},{"location":"data_logger/secret_scanning/#overview","title":"Overview","text":"<p>This dataset contains information about the secret scanning alerts within the organisation. The dataset does not contain any sensitive information such as the content of the alerts, only that there is an alert and the repository it is associated with. Secret scanning alerts must be past a threshold to be included in this dataset. The threshold for alerts is defined in the Data Logger's configuration file (See Configuration for more).</p> <p>This dataset includes all open alerts within the threshold, regardless of whether the repository is archived or not. You can filter alerts by this in the frontend. Information about whether an alert's associated repository is archived and its visibility is collected in the frontend rather than here (See Repository Data Collection for more).</p>"},{"location":"data_logger/secret_scanning/#structure","title":"Structure","text":"<pre><code>[    \n    {\n        \"repository\": \"{repo}\",\n        \"repository_url\": \"https://github.com/{org}/{repo}\",\n        \"creation_date\": \"2024-12-11T23:54:00Z\",\n        \"alert_url\": \"https://github.com/{org}/{repo}/security/secret-scanning/{alert_id}\"\n    },\n]\n</code></pre>"},{"location":"data_logger/threading/","title":"Threading","text":""},{"location":"data_logger/threading/#overview","title":"Overview","text":"<p>The Data Logger uses a modified version of the <code>threading</code> library to perform operations in parallel, allowing it to collect data from the GitHub API quicker than if it were to run sequentially. This is a crucial aspect of the Data Logger's design, as without it, the Data Logger would take significantly longer than AWS Lambda's maximum execution time of 15 minutes.</p> <p>Within this repository, the <code>threading</code> library has been tweaked so that threads can return the values of the operations they perform. Each thread has a <code>return_value</code> attribute that gets set to the output of the function it runs. Without this functionality, threads would not be able to return the data they collect, which is essential for the Data Logger to function correctly.</p>"},{"location":"data_logger/threading/#how-it-works","title":"How it works","text":"<p>In order to make good use of threading, the Data Logger gets given a defined number of threads to use when collecting data. This is set within the configuration file for the tool (see Configuration for more details). The tool implements data parallelism to do the same tasks on many threads. The volume of data being assigned to each thread is determined differently depending on the type of data being collected.</p>"},{"location":"data_logger/threading/#repositoriesjson","title":"<code>repositories.json</code>","text":"<p>When collecting the repository data, each repository is ran through a series of checks, making use of multiple API endpoints. Each thread is assigned a batch of repositories to process, and each thread will return a list of dictionaries containing the data for each repository it processed. Once all processing is complete, the Data Logger will combine the results from all threads into a single list of dictionaries, which is then written to the <code>repositories.json</code> file.</p> <p>This process uses the maximum number of threads specified in the configuration file. Before starting the threads, the Data Logger will collect the total number of repositories to process and divide them into batches based on the number of threads available. Each thread will then process its assigned batch of repositories concurrently.</p> <p>Collecting repository data is the most time-consuming operation in the Data Logger, due to the number of API calls required for each repository.</p>"},{"location":"data_logger/threading/#dependabotjson","title":"<code>dependabot.json</code>","text":"<p>When collecting Dependabot data, the Data Logger will assign a single thread to each severity of Dependabot alert. This means that only 4 threads will be used for this operation, regardless of the number of threads specified in the configuration file. A thread will be created for each of the following severities:</p> <ul> <li>Critical</li> <li>High</li> <li>Medium</li> <li>Low</li> </ul> <p>Each thread will then collect the Dependabot alerts for its assigned severity and return a list of dictionaries containing the data for each alert. Once all threads have completed, the Data Logger will combine the results from all threads into a single list of dictionaries, which is then written to the <code>dependabot.json</code> file.</p> <p>There is plenty of opportunity to improve the performance of this operation in the future, as it is currently limited to 4 threads. The performance of this operation is, for the time being, acceptable, as the time taken to collect Dependabot data is significantly less than the time taken to collect repository data.</p> <p>A better approach to this operation would be to understand the proportion of each severity of Dependabot alert within the organisation and scale the number of threads used for each severity accordingly.</p>"}]}